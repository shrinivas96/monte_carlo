import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from filterpy import monte_carlo                        # for resampling 
import samples_and_measurements as sm                   # custom script to load data from the csv
from matplotlib.animation import FuncAnimation

# Global contants needed throughout
deg2rad = np.pi / 180                                   # multiply to convert to radians
rad2deg = 180 / np.pi                                   # multiply to convert to degrees
T = 0.01                                                # sampling time. new measuements at every T seconds
I = np.identity(3)                                      # identity matrix
zero_mean = np.array([0.0, 0.0, 0.0])                   # to construct any N(0, Cov) and draw samples from it 
np.random.seed(0)                                       # ensures we get the same random results each time


def state_likelihood(measurement_t, state_t):
    """
    Returns the multivariate gaussian likelihood of measurement_t given state_t.
    That is, p(z_t | x_t). x_t is the hypothetical state particle.

    Parameters:
    measurement_t: 2x1 vector of the observed measurement
    state_t: 2x1 vector of hypothetical state generated by sampling from the process model 
    
    Returns the likelhood of z_t given x_t:
    L(z_t | x_t) = (2 pi)^(-k/2) * |cov|^(-1/2) * e^{-0.5 * (z - mu)^T . cov^(-1) . (z - mu)}

    where mu is the expected measurement C*x_t
    """
    # constants for the measurement model
    rw = 10 ** (-6)                                 # uncetrainty in gyro measurements
    r_theta = 5 * (10 ** (-5))                      # uncertainty in accel. measurements
    Rd = np.array([[r_theta, 0],                    # measurement covariance
                   [0, rw]])
    C = np.array([[0, 1, 0], [1, 0, 1]])            # measurement matrix


    # obs_noise = np.random.multivariate_normal(expected_zt, Rd)      # drawing sample from ~ N(Cxt, Rd)

    expected_zt = np.dot(C, state_t)                                # expected z_t for each hyp. particle x_t
    k = measurement_t.size                                          # dimension of the vector 
    e1 = (2 * np.pi) ** (-k/2)                                      # first expression of likelihood

    cov_sqrt_det = np.linalg.det(Rd) ** (-1/2)     # square root of the determinant of covariance matrix
    cov_inverse = np.linalg.inv(Rd)                # inverse of the covariance matrix

    z_mean_diff = (measurement_t - expected_zt)                                                  # difference between observed and expected measurement
    z_mean_diff_T = (measurement_t - expected_zt).T                                              # transpose of above
    exponential = np.exp(-0.5 * (z_mean_diff_T @ cov_inverse @ z_mean_diff))    # exponential part of the likelihood function

    likelihood = e1 * cov_sqrt_det * exponential                                # the likelihood L(z_t)
    return likelihood


def sample_motion_model(particle_n):
    """
    Propagates particle_n through the motion model and adds a gaussian noise.
    
    Parameters:
    particle_n: Hypothetical state vector; i.e the nth particle at time t x^{[n]}_t 
    
    Returns xn_t1: state vector at time t+1 x^{[n]}_{t+1}
    """
    # contants and matrices for motion model
    qw = 5                                          # uncertainty in w := dtheta
    qb = 0.01                                       # uncertainty in bias 
    q_01 = (T ** 2) * qw / 2
    Qd = np.array([[T * qw, q_01, 0],               # motion covariance
                   [q_01, (T ** 3) * qw / 3, 0], 
                   [0, 0, T * qb]])
    Ad = np.array([[1, 0, 0], [T, 1, 0], [0, 0, 1]])        # state transition model
    
    motion_noise = np.random.multivariate_normal(zero_mean, Qd)     # adding motion noise ~ N(0, Qd)
    xn_t1 = np.dot(Ad, particle_n.T) + motion_noise                 # hypothetical nth particle state at time t + 1

    return xn_t1


def particle_filter(particle_set_t, measurement_t):
    """
    Bare-bones particle filter algorithm.
    Takes, as input, the particle set from previous time step and returns the particle set for the next time step.

    Parameters:
    particle_set_t: Array of vectors. Each vector is a state hypothesis and so is of the same dimesion
                    as the state itself.

    measurement_t:  Array of the measurements [z_theta, z_dtheta]

    Returns: particle_set_t1: Array of vectors estimated to be the state in next time step
    """

    n_samples, dim = particle_set_t.shape           # the number of particles and dimension of each particle

    weights = []
    pred_state = []
    est_state_set = []

    
    # this for-loop calculates \bar{X_t}, i.e. the predicted belief.
    for n in range(n_samples):
        # predicted motion step:
        xn_t1 = sample_motion_model(particle_set_t[n])

        # measurement correction step:
        weight_xn_t1 = state_likelihood(measurement_t, xn_t1)

        pred_state.append(xn_t1)
        weights.append(weight_xn_t1)

        # print("Obs: {0} \t\tExp: {1} \t\tProb.: {2}".format(z_t, mean, weight_xn_t1))
    
    weights = np.array(weights)
    weights = weights/np.sum(weights)
    pred_state = np.array(pred_state)

    # the resampling step:
    # indices = monte_carlo.residual_resample(weights)
    indices = monte_carlo.stratified_resample(weights)

    for index in indices:
        est_state_set.append(pred_state[index])

    particle_set_t1 = np.array(est_state_set)
    
    return particle_set_t1


"""
for t in time will go in some main function and 
in each time step we will call the particle filter algorithm 
and pass it just the particle set and enw measurement step in that time step.
"""


# TODO: 1. Move weighting process before prediction to go ahead with first un-intialized particles
# TODO: 2. Make weight and state matrices and then edit them only rather than appending to a new one.


if __name__ == "__main__":
    a, b = -3, 3                                                # interval to generate uniform particles
    n_samples = 5000                                           # number of samples
    dim = 3                                                         # vector dimension for 3 states to be estimated
    particle_set = sm.uniform_samples(a, b, n_samples, dim)         # initialising particles to span the limits of state

    # Actual data taken from the csv
    z_theta, z_dtheta = sm.retreive_data(0)                     # unit rad, rad/sec
    theta_true, dtheta_true = sm.retreive_data(1)               # unit rad, rad/sec
    time = len(z_theta)                                         # total number of measurements that we have

    for t in range(time):
        z_t = np.array([z_theta[t], z_dtheta[t]])
        particle_set = particle_filter(particle_set, z_t)
